import json
from dataclasses import dataclass
from functools import cached_property
from pathlib import Path
from typing import Literal

import numpy as np
import torch
import torchvision.transforms as tf
from einops import repeat
from jaxtyping import Float
from PIL import Image
from torch import Tensor
from torch.utils.data import IterableDataset

from ..geometry.projection import get_fov
from .dataset import DatasetCfgCommon
from .shims.augmentation_shim import apply_augmentation_shim
from .shims.crop_shim import apply_crop_shim
from .types import Stage
from .view_sampler import ViewSampler


@dataclass
class DatasetNerfstudioCfg(DatasetCfgCommon):
    """Configuration for Nerfstudio dataset."""
    
    name: Literal["nerfstudio"]
    """Dataset name identifier."""
    
    transforms_dir: Path
    """Path to directory containing transforms_train.json and transforms_val.json files."""
    
    images_dir: Path
    """Path to directory containing the actual image files."""
    
    max_fov: float = 120.0
    """Maximum field of view in degrees to filter out wide-angle cameras."""
    
    augment: bool = True
    """Whether to apply data augmentation during training."""
    
    test_len: int = -1
    """Number of test examples (-1 for all)."""
    
    train_times_per_scene: int = 1
    """How many times to iterate over each training scene per epoch."""
    
    skip_bad_shape: bool = True
    """Whether to skip images with unexpected shapes."""
    
    near: float = 0.1
    """Near plane distance."""
    
    far: float = 1000.0
    """Far plane distance."""
    
    shuffle_val: bool = True
    """Whether to shuffle validation data."""


class DatasetNerfstudio(IterableDataset):
    """Dataset for loading nerfstudio format JSON files.
    
    This dataset loads data from nerfstudio-style JSON transform files that are
    generated by the VideoCameraTrainerConfig._create_transform_json_for_split method
    in hax_3dllm/models/nerfstudio/configs.py.
    
    Usage example:
    ```python
    from pathlib import Path
    from .view_sampler import ViewSamplerEvaluationCfg, ViewSamplerEvaluation
    
    # Create dataset config
    cfg = DatasetNerfstudioCfg(
        name="nerfstudio",
        transforms_dir=Path("/path/to/transforms"),  # Contains transforms_train.json, transforms_val.json
        images_dir=Path("/path/to/images"),          # Contains the actual image files
        image_shape=[256, 256],                      # Target image shape
        background_color=[0.0, 0.0, 0.0],           # Black background
        cameras_are_circular=False,                  # Whether cameras are in circular motion
        overfit_to_scene=None,                       # Set to scene name to overfit
        view_sampler=ViewSamplerEvaluationCfg(
            name="evaluation",
            num_context_views=2,
            num_target_views=1,
        ),
        max_fov=120.0,                               # Filter out wide-angle cameras
        augment=True,                                # Apply data augmentation
    )
    
    # Create view sampler
    view_sampler = ViewSamplerEvaluation(cfg.view_sampler, "train", False, False, None)
    
    # Create dataset
    dataset = DatasetNerfstudio(cfg, "train", view_sampler)
    
    # Use with DataLoader
    from torch.utils.data import DataLoader
    dataloader = DataLoader(dataset, batch_size=1, num_workers=4)
    ```
    """
    
    cfg: DatasetNerfstudioCfg
    stage: Stage
    view_sampler: ViewSampler
    
    to_tensor: tf.ToTensor
    near: float
    far: float
    
    def __init__(
        self,
        cfg: DatasetNerfstudioCfg,
        stage: Stage,
        view_sampler: ViewSampler,
    ) -> None:
        super().__init__()
        self.cfg = cfg
        self.stage = stage
        self.view_sampler = view_sampler
        self.to_tensor = tf.ToTensor()
        self.near = cfg.near
        self.far = cfg.far
        
        # Load transform data for the current stage
        self._load_transform_data()
    
    def _load_transform_data(self) -> None:
        """Load the transform JSON data for the current stage."""
        # Map stages to transform file names
        stage_to_file = {
            "train": "transforms_train.json",
            "val": "transforms_val.json", 
            "test": "transforms_test.json"
        }
        
        # Use val transforms for test if test doesn't exist
        transform_file = stage_to_file.get(self.stage, "transforms_val.json")
        transform_path = self.cfg.transforms_dir / transform_file
        
        # Fallback to val if test file doesn't exist
        if not transform_path.exists() and self.stage == "test":
            transform_path = self.cfg.transforms_dir / "transforms_val.json"
        
        if not transform_path.exists():
            raise FileNotFoundError(f"Transform file not found: {transform_path}")
        
        with open(transform_path, 'r') as f:
            self.transform_data = json.load(f)
    
    @cached_property
    def scene_data(self) -> dict:
        """Parse the transform data into a scene format."""
        frames = self.transform_data["frames"]
        
        # Extract camera intrinsics (shared across all frames)
        intrinsics = np.eye(3, dtype=np.float32)
        intrinsics[0, 0] = self.transform_data["fl_x"]  # fx
        intrinsics[1, 1] = self.transform_data["fl_y"]  # fy
        intrinsics[0, 2] = self.transform_data["cx"]    # cx
        intrinsics[1, 2] = self.transform_data["cy"]    # cy
        
        # Get image dimensions
        image_width = self.transform_data["w"]
        image_height = self.transform_data["h"]
        
        # Collect all camera data
        cameras = []
        image_paths = []
        
        for frame in frames:
            # Extract camera-to-world transformation matrix
            c2w_matrix = np.array(frame["transform_matrix"], dtype=np.float32)
            
            # Convert to world-to-camera (extrinsics)
            w2c_matrix = np.linalg.inv(c2w_matrix)
            
            cameras.append({
                "extrinsics": w2c_matrix,  # 4x4 world-to-camera matrix
                "intrinsics": intrinsics,  # 3x3 intrinsics matrix  
                "image_width": image_width,
                "image_height": image_height,
            })
            
            # Handle file path - add extension if missing
            file_path = frame["file_path"]
            if not any(file_path.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']):
                # Try common extensions
                for ext in ['.jpg', '.png', '.jpeg']:
                    full_path = self.cfg.images_dir / f"{file_path}{ext}"
                    if full_path.exists():
                        file_path = str(full_path)
                        break
                else:
                    file_path = str(self.cfg.images_dir / f"{file_path}.jpg")  # Default fallback
            else:
                file_path = str(self.cfg.images_dir / file_path)
            
            image_paths.append(file_path)
        
        return {
            "cameras": cameras,
            "image_paths": image_paths,
            "scene_name": f"nerfstudio_{self.stage}",
        }
    
    def shuffle(self, lst: list) -> list:
        """Shuffle a list randomly."""
        indices = torch.randperm(len(lst))
        return [lst[x] for x in indices]
    
    def __iter__(self):
        """Iterate over the dataset."""
        scene_data = self.scene_data
        cameras = scene_data["cameras"]
        image_paths = scene_data["image_paths"]
        scene_name = scene_data["scene_name"]
        
        # Create indices for frames
        frame_indices = list(range(len(cameras)))
        
        # Shuffle for training and validation if configured
        if self.stage in (("train", "val") if self.cfg.shuffle_val else ("train",)):
            frame_indices = self.shuffle(frame_indices)
        
        # Convert camera data to tensors
        extrinsics_list = []
        intrinsics_list = []
        
        for camera in cameras:
            extrinsics_list.append(torch.from_numpy(camera["extrinsics"]).float())
            intrinsics_list.append(torch.from_numpy(camera["intrinsics"]).float())
        
        extrinsics = torch.stack(extrinsics_list)  # [num_frames, 4, 4]
        intrinsics = torch.stack(intrinsics_list)  # [num_frames, 3, 3]
        
        # Determine how many times to iterate over the scene
        times_per_scene = (
            1 if self.stage == "test" else self.cfg.train_times_per_scene
        )
        
        # If test_len is specified and we're testing, limit the number of iterations
        total_iterations = int(times_per_scene * len(frame_indices))
        if self.stage == "test" and self.cfg.test_len > 0:
            total_iterations = min(total_iterations, self.cfg.test_len)
        
        for _ in range(total_iterations):
            try:
                # Sample context and target views
                context_indices, target_indices = self.view_sampler.sample(
                    scene_name,
                    extrinsics,
                    intrinsics,
                )
            except ValueError:
                # Skip if we can't sample enough views
                continue
            
            # Check field of view constraints
            if (get_fov(intrinsics).rad2deg() > self.cfg.max_fov).any():
                continue
            
            # Load images for context and target views
            try:
                context_images = self._load_images([image_paths[idx.item()] for idx in context_indices])
                target_images = self._load_images([image_paths[idx.item()] for idx in target_indices])
            except (FileNotFoundError, OSError) as e:
                print(f"Skipping iteration due to image loading error: {e}")
                continue
            
            # Skip if images have bad shapes
            if self.cfg.skip_bad_shape:
                expected_shape = context_images.shape[1:]  # [C, H, W]
                if (context_images.shape[1:] != expected_shape or 
                    target_images.shape[1:] != expected_shape):
                    print(f"Skipping bad shape - context: {context_images.shape}, target: {target_images.shape}")
                    continue
            
            # Create the example in the expected format
            example = {
                "context": {
                    "extrinsics": extrinsics[context_indices],
                    "intrinsics": intrinsics[context_indices], 
                    "image": context_images,
                    "near": self.get_bound("near", len(context_indices)),
                    "far": self.get_bound("far", len(context_indices)),
                    "index": context_indices,
                },
                "target": {
                    "extrinsics": extrinsics[target_indices],
                    "intrinsics": intrinsics[target_indices],
                    "image": target_images, 
                    "near": self.get_bound("near", len(target_indices)),
                    "far": self.get_bound("far", len(target_indices)),
                    "index": target_indices,
                },
                "scene": scene_name,
            }
            
            # Apply augmentation for training
            if self.stage == "train" and self.cfg.augment:
                example = apply_augmentation_shim(example)
            
            # Apply cropping to match target image shape
            yield apply_crop_shim(example, tuple(self.cfg.image_shape))
    
    def _load_images(self, image_paths: list[str]) -> Float[Tensor, "batch 3 height width"]:
        """Load and convert images to tensors."""
        torch_images = []
        
        for image_path in image_paths:
            try:
                # Load image
                image = Image.open(image_path).convert('RGB')
                
                # Convert to tensor
                torch_image = self.to_tensor(image)
                torch_images.append(torch_image)
                
            except Exception as e:
                raise FileNotFoundError(f"Could not load image {image_path}: {e}")
        
        return torch.stack(torch_images)
    
    def get_bound(
        self,
        bound: Literal["near", "far"], 
        num_views: int,
    ) -> Float[Tensor, " view"]:
        """Get near or far bounds for the specified number of views."""
        value = torch.tensor(getattr(self, bound), dtype=torch.float32)
        return repeat(value, "-> v", v=num_views)
    
    def __len__(self) -> int:
        """Get the dataset length."""
        scene_data = self.scene_data
        base_length = len(scene_data["cameras"])
        
        if self.stage == "test" and self.cfg.test_len > 0:
            return min(base_length, self.cfg.test_len)
        
        return base_length * self.cfg.train_times_per_scene
